{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_cats = ['/home/shubham/all/train/'+filename for filename in os.listdir(\"/home/shubham/all/train\") if filename.startswith(\"cat\")]\n",
    "filenames_dogs = ['/home/shubham/all/train/'+filename for filename in os.listdir(\"/home/shubham/all/train\") if filename.startswith(\"dog\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/shubham/all/train/cat.4606.jpg'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = filenames_cats+filenames_dogs\n",
    "filenames[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [0 for _ in range(len(filenames_cats))]+[1 for _ in range(len(filenames_dogs))]\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filenames = tf.constant(filenames)\n",
    "#labels = tf.constant(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print tf.shape(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Shuffling the datset \n",
    "dataset = dataset.shuffle(len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    \n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "\n",
    "    #one_hot = tf.one_hot(label, 2)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "    image = tf.image.resize_images(image, [64, 64])\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(parse_function , num_parallel_calls = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data augmentation \n",
    "def train_preprocess(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)\n",
    "    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "\n",
    "    # Make sure the image is still in [0, 1]\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(train_preprocess, num_parallel_calls=50)\n",
    "#dataset = dataset.concatenate(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.batch(batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iter = dataset.make_one_shot_iterator()\n",
    "x,y = iter.get_next()\n",
    "# with tf.Session() as sess:\n",
    "#     for i in range(25000):\n",
    "#         sess.run(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev= 0.1))\n",
    "\n",
    "def init_bias(shape):\n",
    "    return tf.Variable(tf.constant(0.1,shape=shape))\n",
    "\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides = [1,1,1,1],padding=\"SAME\")\n",
    "\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x,ksize = [1,2,2,1],strides = [1,2,2,1], padding =\"SAME\")\n",
    "\n",
    "def convolution_layer(input_x,shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x,W) + b)\n",
    "\n",
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84837097\n",
      "0.49\n",
      "0.79326165\n",
      "0.465\n",
      "0.7882617\n",
      "0.5\n",
      "0.77826154\n",
      "0.565\n",
      "0.80326164\n",
      "0.505\n",
      "0.83326155\n",
      "0.54\n",
      "0.82826155\n",
      "0.54\n",
      "0.84326166\n",
      "0.505\n",
      "0.8232616\n",
      "0.505\n",
      "0.75326174\n",
      "0.445\n",
      "0.78326166\n",
      "0.555\n",
      "0.8182616\n",
      "0.51\n",
      "0.90326154\n",
      "0.475\n",
      "0.7982616\n",
      "0.51\n",
      "0.7782617\n",
      "0.465\n",
      "0.77826154\n",
      "0.515\n",
      "0.82326156\n",
      "0.455\n",
      "0.83326155\n",
      "0.475\n",
      "0.8082616\n",
      "0.53\n",
      "0.83326155\n",
      "0.53\n",
      "0.7782617\n",
      "0.47\n",
      "0.80826163\n"
     ]
    }
   ],
   "source": [
    "##Defining the VGG Block\n",
    "x = tf.reshape(x,shape = [-1,64,64,3])\n",
    "convo_1 = convolution_layer(x,shape=[3,3,3,64])\n",
    "convo_2 = convolution_layer(convo_1,shape=[3,3,64,64])\n",
    "pool_1 = max_pool(convo_2)\n",
    "fc = tf.reshape(pool_1,shape = [-1,32*32*64])\n",
    "fc_1 = normal_full_layer(fc,1000)\n",
    "fc_1 = tf.nn.relu(fc_1)\n",
    "fc_2 = normal_full_layer(fc_1,2)\n",
    "\n",
    "logits = tf.nn.softmax(fc_2)\n",
    "\n",
    "labels = y\n",
    "labels = tf.cast(labels, tf.int64)\n",
    "tf.shape(y)\n",
    "# Define the prediction as the argmax of the scores\n",
    "predictions = tf.argmax(logits, 1)\n",
    "\n",
    "# Define the loss\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "# Create an optimizer that will take care of the Gradient Descent\n",
    "optimizer = tf.train.AdamOptimizer(0.1)\n",
    "\n",
    "matches = tf.equal(predictions,labels)\n",
    "acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "\n",
    "# Create the training operation\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(100):\n",
    "        _, loss_val = sess.run([train_op, loss])\n",
    "        print loss_val\n",
    "        print sess.run(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''The below code is using the inbuilt layer functions of tensorflow'''\n",
    "\n",
    "# out = x\n",
    "# out = tf.layers.conv2d(out, 64, 3, padding='same')\n",
    "# out = tf.nn.relu(out)\n",
    "# out = tf.layers.conv2d(out, 64, 3, padding='same')\n",
    "# out = tf.layers.max_pooling2d(out, 2, 2)\n",
    "\n",
    "# # First, reshape the output into [batch_size, flat_size]\n",
    "# out = tf.reshape(out, [-1, 32 * 32 * 64])\n",
    "# # Now, logits is [batch_size, 6]\n",
    "# logits = tf.layers.dense(out,1000)\n",
    "# logits = tf.nn.relu(logits)\n",
    "# logits = tf.layers.dense(logits,2)\n",
    "# logits = tf.nn.softmax(logits)\n",
    "\n",
    "# labels = y\n",
    "# labels = tf.cast(labels, tf.int64)\n",
    "\n",
    "# # Define the prediction as the argmax of the scores\n",
    "# predictions = tf.argmax(logits, 1)\n",
    "\n",
    "# # Define the loss\n",
    "# loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "# # Create an optimizer that will take care of the Gradient Descent\n",
    "# optimizer = tf.train.AdamOptimizer(0.001)\n",
    "\n",
    "# matches = tf.equal(predictions,labels)\n",
    "# acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "\n",
    "# # Create the training operation\n",
    "# train_op = optimizer.minimize(loss)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     for i in range(100):\n",
    "#         _, loss_val = sess.run([train_op, loss])\n",
    "#         print loss_val\n",
    "#         print sess.run(acc)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
