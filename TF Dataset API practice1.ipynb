{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xc but this version of numpy is 0xb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xc but this version of numpy is 0xb"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xc but this version of numpy is 0xb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xc but this version of numpy is 0xb"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "os.chdir('/home/himanshu/cats_dogs/')\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Obtaining image file names from local directory\n",
    "IMAGE_DIR_PATH = '/home/himanshu/cats_dogs/train'\n",
    "image_paths = [os.path.join(IMAGE_DIR_PATH, x) for x in os.listdir(IMAGE_DIR_PATH) if x.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Obtaining labels for the whole data set of 25000 images\n",
    "labels = []\n",
    "for x in os.listdir(IMAGE_DIR_PATH):\n",
    "    if x.startswith('cat'):\n",
    "        label = 0\n",
    "    elif x.startswith('dog'):\n",
    "        label = 1\n",
    "        \n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Splitting the training and validation dataset\n",
    "\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(pd.DataFrame(image_paths), pd.DataFrame(labels),test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_path = np.asarray(x_train[:4096]).reshape(4096,)\n",
    "y_path = np.asarray(y_train[:4096]).reshape(4096,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'x_train = %s, y_train = %s' %(x_path.shape,y_path.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Training Parameters \n",
    "learning_rate = 0.001\n",
    "num_epochs = 4\n",
    "w = 64      ## height and width of image\n",
    "l = 3       ## num of channels in input image\n",
    "batch_size = 16\n",
    "display_step = 10\n",
    "# epsilon = 10e-8\n",
    "\n",
    "## Network Parameters\n",
    "num_classes = 2     # Num of classes in data\n",
    "dropout = 0.50        # Dropout, probability to keep units\n",
    "\n",
    "## tf Graph input\n",
    "# X = tf.placeholder(tf.float32, shape = [None,w,w,l])\n",
    "# Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "# phase = tf.placeholder(tf.bool, name='phase')\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining augmentation and data parsing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_data(image_paths, label):\n",
    "    image_content = tf.read_file(image_paths)\n",
    "    images = tf.image.decode_jpeg(image_content, channels=3)\n",
    "    return images, label\n",
    "\n",
    "def bright(image, label):\n",
    "    cond_brightness = tf.cast(tf.random_uniform([], maxval=2, dtype=tf.int32), tf.bool)\n",
    "    br_image = tf.cond(cond_brightness, lambda: tf.image.random_hue(image, 0.1), lambda: tf.identity(image))\n",
    "    return br_image, label\n",
    "\n",
    "def contrast(image, label):\n",
    "    cond_contrast = tf.cast(tf.random_uniform([], maxval=2, dtype=tf.int32), tf.bool)\n",
    "    con_image = tf.cond(cond_contrast, lambda: tf.image.random_contrast(image, 0.2, 1.8), lambda: tf.identity(image))\n",
    "    return con_image, label\n",
    "\n",
    "def flipping(image, label):\n",
    "    seed = random.random()\n",
    "    flip_image = tf.image.random_flip_left_right(image, seed=seed)\n",
    "    return flip_image, label\n",
    "\n",
    "def preprocess(image,label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image/255.0\n",
    "    image = tf.image.resize_images(image, [w, w])\n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating batch input data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_input(x_path, y_path, Augment=False, num_threads=4):\n",
    "    \n",
    "    ## Creating a tensor of image paths\n",
    "    img_path_tensors = tf.constant(x_path)\n",
    "    label_tensors = tf.constant(y_path)\n",
    "    \n",
    "    ## Creating dataset out of the tensor file\n",
    "    path_data = tf.data.Dataset.from_tensor_slices((img_path_tensors,label_tensors))\n",
    "    \n",
    "    ## Parsing the dataset\n",
    "    data = path_data.map(parse_data, num_parallel_calls=num_threads)\n",
    "    \n",
    "    ## Normalizing and resizing the training data\n",
    "    data = data.map(preprocess, num_parallel_calls=num_threads)  \n",
    "    \n",
    "    ## If augmentation is to be applied\n",
    "    if Augment:\n",
    "        br_data = data.map(bright, num_parallel_calls=num_threads)      ## Brightness augmentation\n",
    "        \n",
    "        con_data = data.map(contrast, num_parallel_calls=num_threads)   ## Contrast augmentation\n",
    "        \n",
    "        flip_data = data.map(flipping, num_parallel_calls=num_threads)  ## Flip augmentation\n",
    "        \n",
    "    ## Concatenating the original and augmented data\n",
    "    train_data = data.concatenate(br_data)\n",
    "    train_data = train_data.concatenate(con_data)\n",
    "    train_data = train_data.concatenate(flip_data)\n",
    "    \n",
    "    train_data = train_data.shuffle(16384)\n",
    "    \n",
    "    X = train_data.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    ## Create iterator\n",
    "    iterator = X.make_one_shot_iterator()\n",
    "\n",
    "    ## Next element Op\n",
    "    x_batch, y_batch = iterator.get_next()\n",
    "\n",
    "    ## Data set init. op\n",
    "#     init_op = iterator.make_initializer(X)\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining training and network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_batch, y_batch = batch_input(x_path, y_path, Augment=True, num_threads=4)\n",
    "\n",
    "conv1 = tf.layers.conv2d(x_batch, 16, 3, padding = 'SAME', activation=tf.nn.relu)\n",
    "\n",
    "conv2 = tf.layers.conv2d(conv1, 32, 3, padding = 'SAME', activation=tf.nn.relu)\n",
    "conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "## Fully connected layers\n",
    "## Reshaping the output from 3D image shape to 1D vectors for fully connected layers\n",
    "max_pool = tf.layers.flatten(conv2)\n",
    "fc1 = tf.layers.dense(max_pool, 128, activation=tf.nn.relu)\n",
    "fc1 = tf.nn.dropout(fc1, dropout)         ## Applying Dropout\n",
    "\n",
    "fc2 = tf.layers.dense(fc1, 1)\n",
    "logits = tf.nn.relu(fc2)\n",
    "\n",
    "\n",
    "## Obtaining predictions using softmax layer\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "## Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_batch))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_op)\n",
    "\n",
    "## Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_batch, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "## Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "tf.summary.scalar('accuracy',accuracy)\n",
    "tf.summary.scalar('loss',loss_op)\n",
    "merged = tf.summary.merge_all()\n",
    "    \n",
    "writer = tf.summary.FileWriter('/home/himanshu/cats_dogs/', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\"tf.set_random_seed(42)\n",
    "## Initializing layer weights & biases\n",
    "weights = {\n",
    "    # 3x3 conv, 1 input, 64 outputs\n",
    "    'W1': tf.Variable(tf.random_normal([3, 3, 3, 64], mean=0, stddev=0.1), name='W1'),\n",
    "    # 3x3 conv, 64 inputs, 64 outputs\n",
    "    'W2': tf.Variable(tf.random_normal([3, 3, 64, 64], mean=0, stddev=0.1), name='W2'),\n",
    "    # fully connected, 32*32*64 inputs, 4096 outputs\n",
    "    'fw1': tf.Variable(tf.random_normal([32*32*64, 4096]),name='fw1'),\n",
    "    # fully connected, 4096 inputs, 1000 outputs\n",
    "    'fw2': tf.Variable(tf.random_normal([4096, 1000]),name='fw2'),\n",
    "    # 1000 inputs, 2 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1000, num_classes]),name='out')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([64]),name='b1'),\n",
    "    'b2': tf.Variable(tf.random_normal([64]),name='b2'),\n",
    "    'fb1': tf.Variable(tf.random_normal([4096]),name='fb1'),\n",
    "    'fb2': tf.Variable(tf.random_normal([1000]),name='fb2'),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]),name='out')\n",
    "}\"\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Initiation Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    " \n",
    "    ## Run the initializer\n",
    "#     sess.run(init_op)\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for step in range(1, int((len(x_path)/batch_size)+1)):\n",
    "            ## Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={keep_prob: dropout})\n",
    "            if step % display_step == 0 or step == 1:\n",
    "                ## Calculate batch loss and accuracy\n",
    "                loss, acc,summary = sess.run([loss_op,accuracy,merged], feed_dict={keep_prob: 1.0})\n",
    "                writer.add_summary(summary,step)\n",
    "                print(\"Epoch \" + str(epoch)+\", Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.3f}\".format(acc))\n",
    "\n",
    "                ##saving checkpoints after every 10 iterations\n",
    "                save_path = tf.train.Saver().save(sess, '/home/himanshu/cats_dogs/chkpts.ckpt', global_step=step)\n",
    "\n",
    "   \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
